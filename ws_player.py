import base64
import io
import json
import logging
from queue import Queue
import socket
import threading
import wave
import pygame
import time
import asyncio
import websockets

from livelink.connect.livelink_init import create_socket_connection, initialize_py_face
from livelink.animations.default_animation import default_animation_loop, stop_default_animation

from livelink.send_to_unreal import pre_encode_facial_data, pre_encode_facial_data_blend_in, pre_encode_facial_data_blend_out, send_pre_encoded_data_to_unreal
from utils.audio.convert_audio import pcm_to_wav
from utils.audio.play_audio import init_pygame_mixer, play_audio_from_memory, simple_playback_loop, sync_playback_loop
from utils.audio_face_workers import audio_face_queue_worker, log_timing_worker


def compute_min_buffer_size(realtime_config):
    """
    Compute the minimum buffer size based on realtime configuration parameters.
    """
    # Retrieve configuration values with defaults
    sample_rate = realtime_config.get("sample_rate", 22050)
    channels = realtime_config.get("channels", 1)
    sample_width = realtime_config.get("sample_width", 2)
    min_buffer_duration = realtime_config.get("min_buffer_duration", 5)
    # Compute and return the minimum buffer size needed
    return int(min_buffer_duration * sample_rate * channels * sample_width)


# é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨ï¼Œè®¾ç½®è°ƒè¯•çº§åˆ«
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s.%(msecs)03d - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# å•ç‹¬ä¸º httpx å’Œ httpcore æ¨¡å—è®¾ç½®è°ƒè¯•çº§åˆ«
logging.getLogger("httpx").setLevel(logging.DEBUG)
logging.getLogger("httpcore").setLevel(logging.DEBUG)

# é…ç½®å®æ—¶å‚æ•°ï¼ˆç”¨äºè½¬æ¢å¤„ç†ï¼‰
realtime_config = {
    "min_buffer_duration": 2,
    "sample_rate": 24000,
    "channels": 1,
    "sample_width": 2
}


# debug method to save the last audio recieved from client side as a wave file
def save_audio_to_wav(pcm_data: bytes, filename="debug_audio.wav", sample_rate=24000):
    """
    ä¿å­˜ PCM s16le æ•°æ®åˆ° WAV æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(1)  # å•å£°é“
            wf.setsampwidth(2)  # 16-bit PCM æ¯ä¸ªæ ·æœ¬ 2 å­—èŠ‚
            wf.setframerate(sample_rate)  # é‡‡æ ·ç‡
            wf.writeframes(pcm_data)

        logging.info(f"ğŸ“ éŸ³é¢‘å·²ä¿å­˜: {filename}")
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜ WAV æ–‡ä»¶å¤±è´¥: {e}")

current_websocket = None
ws_event_loop = None  # ç”¨äºä¿å­˜ WebSocket æœåŠ¡å™¨çš„äº‹ä»¶å¾ªç¯
def ws_audio_server(audio_face_queue, host="0.0.0.0", port=8766, debug_queue: Queue=None):
    """
    å¯åŠ¨ä¸€ä¸ª WebSocket æœåŠ¡å™¨ï¼Œç›‘å¬å®¢æˆ·ç«¯ä¸Šä¼ çš„ JSON æ•°æ®ï¼Œ
    åªæœ‰å½“ç´¯è®¡çš„éŸ³é¢‘æ•°æ®è¾¾åˆ° min_buffer_size æ—¶æ‰æ¨é€æ•°æ®åˆ° audio_face_queueã€‚
    """
    global current_websocket, ws_event_loop
    min_buffer_size = compute_min_buffer_size(realtime_config)
    
    async def handler(websocket):
        global current_websocket
        logging.info(f"æ–°å®¢æˆ·ç«¯è¿æ¥: {websocket.remote_address}")
        current_websocket = websocket

        # ç”¨äºç¼“å†²éŸ³é¢‘å’ŒåŠ¨ç”»æ•°æ®
        buffer_audio = bytearray()
        buffer_facial_data = []
        
        try:
            async for message in websocket:
                if isinstance(message, str):
                    if message == '<END>':
                        # æ”¶åˆ° <END> æ¶ˆæ¯æ—¶ï¼Œå…ˆæŠŠå‰©ä½™ç¼“å†²æ•°æ®ï¼ˆä¸è¶³ min_buffer_size çš„éƒ¨åˆ†ï¼‰æ¨é€å‡ºå»
                        if buffer_audio:
                            audio_face_queue.put((bytes(buffer_audio), buffer_facial_data.copy()))
                            buffer_audio.clear()
                            buffer_facial_data.clear()
                        # åŒæ—¶å¤„ç† TTS è°ƒè¯•æ•°æ®
                        audio_bytes_list = []
                        while not debug_queue.empty():
                            audio_bytes_list.append(debug_queue.get())
                            debug_queue.task_done()
                        full_audio_bytes = b"".join(audio_bytes_list)
                        save_audio_to_wav(full_audio_bytes, filename="output_tts.wav")
                        logging.info("TTS è¾“å‡ºå·²å­˜æˆæ–‡ä»¶ï¼šoutput_tts.wav")
                    else:
                        data = json.loads(message)
                        audio_bytes = base64.b64decode(data["audio"])
                        logging.info("æ”¶åˆ°TTSè¯­éŸ³: %s bytes", len(audio_bytes))

                        blendshapes = data["blendshapes"]
                        logging.info("æ”¶åˆ°blendshapesåŠ¨ç”»: %s elements", len(blendshapes))

                        facial_data = []
                        for frame in blendshapes:
                            # å°†æ¯ä¸€å¸§çš„æ•°æ®è½¬ä¸º float æ•°ç»„
                            frame_data = [float(value) for value in frame]
                            facial_data.append(frame_data)

                        # å°†æœ¬æ¬¡æ¥æ”¶çš„æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
                        buffer_audio.extend(audio_bytes)
                        # å‡è®¾ facial_data ä¸ºåˆ—è¡¨ï¼Œç›´æ¥æ‰©å±•ç¼“å†²åŒº
                        buffer_facial_data.extend(facial_data)

                        # åŒæ—¶å°†è°ƒè¯•éŸ³é¢‘æ•°æ®å­˜å…¥ debug_queue
                        debug_queue.put(audio_bytes)

                        # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦å·²è¾¾åˆ°æœ€å°è¦æ±‚
                        if len(buffer_audio) >= min_buffer_size:
                            # è¾¾åˆ°ç¼“å†²è¦æ±‚åï¼Œå°†ç¼“å†²æ•°æ®æ‰“åŒ…æ¨é€åˆ° audio_face_queue
                            audio_face_queue.put((bytes(buffer_audio), buffer_facial_data.copy()))
                            # æ¸…ç©ºç¼“å†²åŒºï¼Œç­‰å¾…ä¸‹æ¬¡ç§¯ç´¯
                            buffer_audio.clear()
                            buffer_facial_data.clear()
                else:
                    logging.info("æ”¶åˆ°éä¸šåŠ¡æ¶ˆæ¯: %s", message)
        except websockets.ConnectionClosed:
            logging.info("å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
        finally:
            # å¦‚æœ websocket å…³é—­åï¼Œä»æœ‰å‰©ä½™æ•°æ®ï¼Œå¯ä»¥é€‰æ‹©æ¨é€å‡ºå»
            if buffer_audio:
                audio_face_queue.put((bytes(buffer_audio), buffer_facial_data.copy()))
                buffer_audio.clear()
                buffer_facial_data.clear()

    async def server_main():
        server = await websockets.serve(handler, host, port)
        logging.info(f"WebSocketæœåŠ¡å™¨å¯åŠ¨ï¼Œç›‘å¬ {host}:{port}")
        await server.wait_closed()

    # åˆ›å»ºå¹¶è®¾ç½®ä¸€ä¸ªæ–°çš„äº‹ä»¶å¾ªç¯ï¼Œç”¨äº WebSocket æœåŠ¡å™¨
    ws_event_loop = asyncio.new_event_loop()
    asyncio.set_event_loop(ws_event_loop)
    ws_event_loop.run_until_complete(server_main())
    
def tcp_listener(events_queue):
    host = '0.0.0.0'  # ç›‘å¬æ‰€æœ‰ç½‘å¡
    port = 7777

    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind((host, port))
        s.listen()
        print(f"TCPListener å·²åœ¨ç«¯å£ {port} å¯åŠ¨ï¼Œç­‰å¾…è¿æ¥...")

        while True:
            conn, addr = s.accept()  # æ¥å—æ–°çš„è¿æ¥
            print(f"è¿æ¥æ¥è‡ª: {addr}")
            with conn:
                data = conn.recv(1024)
                if data:
                    text = data.decode('utf-8')
                    print("æ”¶åˆ°æ–‡æœ¬:", text)
                    if text == "startspeaking":
                        events_queue.put("ANIM_START")
                    elif text == "stopspeaking":
                        events_queue.put("ANIM_END")
                    conn.sendall(b"ACK")
                # ç¦»å¼€withå—åï¼Œè¿æ¥ä¼šè‡ªåŠ¨å…³é—­


def events_dispatcher(events_queue):
    global current_websocket, ws_event_loop
    while True:
        event = events_queue.get()
        if event is None:
            events_queue.task_done()
            break
        if current_websocket is not None and ws_event_loop is not None:
            # å°†å‘é€æ“ä½œè°ƒåº¦åˆ° WebSocket æœåŠ¡å™¨çš„äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ
            asyncio.run_coroutine_threadsafe(current_websocket.send(event), ws_event_loop)
            logging.info(f"å‘å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯: {event}")
        events_queue.task_done()

queue_lock = threading.Lock()

def accumulate_data(audio_bytes, facial_data, accumulated_audio, accumulated_facial_data, encoded_facial_data, py_face, single_entry=False):
    """
    Accumulates incoming audio and facial data while pre-encoding facial animation.
    
    If single_entry is True (i.e. only one entry is present),
    then we call pre_encode_facial_data (which blends in and out).
    Otherwise, the first entry uses pre_encode_facial_data_blend_in and subsequent entries use pre_encode_facial_data_blend_out.
    """
    with queue_lock:
        accumulated_audio.extend(audio_bytes)
        if len(accumulated_facial_data) == 0:
            if single_entry:
                # Changed: For a single entry, pre-encode with both blend in and out.
                encoded_facial_data.extend(pre_encode_facial_data(facial_data, py_face))
            else:
                encoded_facial_data.extend(pre_encode_facial_data_blend_in(facial_data, py_face))
        else:
            encoded_facial_data.extend(pre_encode_facial_data_blend_out(facial_data, py_face))
        accumulated_facial_data.extend(facial_data)
def playback_loop(stop_worker, start_event, accumulated_audio, encoded_facial_data, audio_face_queue, py_face, socket_connection, default_animation_thread, log_queue, events_queue):
    """
    Continuously plays accumulated audio and encoded facial data.
    """
    while not stop_worker.is_set():
        with queue_lock:
            if not accumulated_audio or not encoded_facial_data:
                time.sleep(0.01)
                continue

            playback_audio = accumulated_audio[:]
            playback_facial_data = encoded_facial_data[:]
            accumulated_audio.clear()
            encoded_facial_data.clear()

        stop_default_animation.set()
        if default_animation_thread and default_animation_thread.is_alive():
            default_animation_thread.join()

        playback_start_time = time.time()
        frame_duration = len(playback_facial_data) / 60
        events_queue.put(f"ANIM_START:{frame_duration}")
        play_audio_and_animation_openai_realtime(playback_audio, playback_facial_data, start_event, socket_connection)
        events_queue.put(f"ANIM_END")
        playback_end_time = time.time()
        log_queue.put(f"Playback duration: {playback_end_time - playback_start_time:.3f} seconds.")

        time.sleep(0.01)

        check_and_restart_default_animation(accumulated_audio, encoded_facial_data, audio_face_queue, py_face)

def play_audio_and_animation_openai_realtime(playback_audio, playback_facial_data, start_event, socket_connection):
    """
    Plays audio and sends animation data using separate threads.
    """
    audio_thread = threading.Thread(target=play_audio_from_memory_openai, args=(playback_audio, start_event))
    data_thread = threading.Thread(target=send_pre_encoded_data_to_unreal, args=(playback_facial_data, start_event, 60, socket_connection))

    audio_thread.start()
    data_thread.start()
    start_event.set()

    audio_thread.join()
    data_thread.join()

def audio_face_queue_worker_realtime_v2(audio_face_queue, events_queue, py_face, socket_connection, default_animation_thread):
    """
    Streams (audio_bytes, facial_data) pairs in real-time.
    - Uses a separate thread to log timing data.
    """
    accumulated_audio = bytearray()
    accumulated_facial_data = []
    encoded_facial_data = []
    stop_worker = threading.Event()
    start_event = threading.Event()

    log_queue = Queue()  # Separate queue for logging
    log_thread = threading.Thread(target=log_timing_worker, args=(log_queue,), daemon=True)
    log_thread.start()

    playback_thread = threading.Thread(
        target=playback_loop,
        args=(
            stop_worker,
            start_event,
            accumulated_audio,
            encoded_facial_data,
            audio_face_queue,
            py_face,
            socket_connection,
            default_animation_thread,
            log_queue,  # Pass the log queue
            events_queue, 
        ),
        daemon=True
    )
    playback_thread.start()

    # Process each item in the queue.
    while True:
        item = audio_face_queue.get()
        if item is None:
            audio_face_queue.task_done()
            break

        received_time = time.time()  # Timestamp when data is received

        audio_bytes, facial_data = item
        audio_face_queue.task_done()

        # Changed: Determine if this is the only entry by checking if the queue is empty.
        single_entry = audio_face_queue.empty()
        accumulate_data(audio_bytes, facial_data, accumulated_audio, accumulated_facial_data, encoded_facial_data, py_face, single_entry)
        playback_start_time = time.time()
        log_queue.put(f"Time from queue reception to addition to encoded queue: {playback_start_time - received_time:.3f} seconds.")

    time.sleep(0.1)
    audio_face_queue.join()

    stop_worker.set()
    playback_thread.join()

    log_queue.put(None)  # Signal log thread to exit
    log_thread.join()

    time.sleep(0.01)
    with queue_lock:
        stop_default_animation.clear()
        new_default_thread = threading.Thread(target=default_animation_loop, args=(py_face,))
        new_default_thread.start()
        
def check_and_restart_default_animation(accumulated_audio, encoded_facial_data, audio_face_queue, py_face):
    """
    Restarts default animation if no data is available.
    """
    with queue_lock:
        if not accumulated_audio and not encoded_facial_data and audio_face_queue.empty():
            stop_default_animation.clear()
            new_default_thread = threading.Thread(target=default_animation_loop, args=(py_face,))
            new_default_thread.start()

def play_audio_from_memory_openai(audio_data, start_event, sync=True):
    """
    Play audio from memory with potential PCM-to-WAV conversion.
    
    If the audio data does not start with the WAV header ('RIFF'), assume
    it is raw PCM and convert it.
    """
    try:
        init_pygame_mixer()
        if not audio_data.startswith(b'RIFF'):
            # Convert raw PCM to WAV using default parameters.
            audio_file = pcm_to_wav(audio_data, sample_rate=24000, channels=1, sample_width=2)
        else:
            audio_file = io.BytesIO(audio_data)
        pygame.mixer.music.load(audio_file)
        start_event.wait()
        pygame.mixer.music.play()
        if sync:
            sync_playback_loop()
        else:
            simple_playback_loop()
    except pygame.error as e:
        print(f"Error in play_audio_from_memory_openai: {e}")

def main():
    # åˆå§‹åŒ–äººè„¸æ¨¡å—ã€socket è¿æ¥å’Œé»˜è®¤åŠ¨ç”»
    py_face = initialize_py_face()
    socket_connection = create_socket_connection()
    default_animation_thread = threading.Thread(target=default_animation_loop, args=(py_face,))
    default_animation_thread.start()

    # å®šä¹‰å„ä¸ªæ•°æ®å¤„ç†é˜Ÿåˆ—
    audio_face_queue = Queue()
    events_queue = Queue()
    debug_queue = Queue()

    # å¯åŠ¨äº‹ä»¶ç›‘æ§çº¿ç¨‹ï¼Œå°†åŠ¨ç”»å¯åœäº‹ä»¶è½¬å‘ç»™å®¢æˆ·ç«¯
    events_dispatcher_thread = threading.Thread(
        target=events_dispatcher,
        args=(events_queue,)
    )
    events_dispatcher_thread.start()

    # å¯åŠ¨ TCP æœåŠ¡å™¨çº¿ç¨‹ï¼Œæ¥æ”¶å®¢æˆ·ç«¯å‘é€çš„åŠ¨ç”»æ§åˆ¶äº‹ä»¶
    # tcp_thread = threading.Thread(target=tcp_listener, args=(events_queue,))
    # tcp_thread.start()

    # å¯åŠ¨éŸ³é¢‘å¤„ç†å·¥ä½œçº¿ç¨‹ï¼šä» audio_face_queue ä¸­è·å–æ•°æ®è¿›è¡Œåç»­å¤„ç†ï¼ˆå¦‚éŸ³é¢‘é©±åŠ¨äººè„¸ï¼‰
    # audio_worker_thread =  threading.Thread(target=audio_face_queue_worker, args=(audio_face_queue, py_face, socket_connection, default_animation_thread, True)) 
    audio_worker_thread = threading.Thread(
        target=audio_face_queue_worker_realtime_v2,
        args=(audio_face_queue, events_queue,  py_face, socket_connection, default_animation_thread)
    )
    audio_worker_thread.start()

    # å¯åŠ¨ WebSocket æœåŠ¡å™¨çº¿ç¨‹ï¼Œæ¥æ”¶å®¢æˆ·ç«¯ä¸Šä¼ çš„éŸ³é¢‘ã€å­—å¹•åŠåŠ¨ç”»æ•°æ®æ•°æ®ï¼Œç›´æ¥æ”¾å…¥ audio_face_queue
    ws_thread = threading.Thread(
        target=ws_audio_server,
        args=(audio_face_queue, "0.0.0.0", 8768, debug_queue),
        daemon=True
    )
    ws_thread.start()

    try:
        logging.info("ç³»ç»Ÿå¯åŠ¨ï¼ŒWebSocketæœåŠ¡å™¨æ­£åœ¨ç›‘å¬å®¢æˆ·ç«¯éŸ³é¢‘æ•°æ®ã€‚æŒ‰ 'Ctrl+C' é”®é€€å‡ºã€‚")
        while True:
            time.sleep(0.1)
    except KeyboardInterrupt:
        logging.info("æ£€æµ‹åˆ°é€€å‡ºæŒ‡ä»¤ (Ctrl+C)ã€‚")
    finally:
        # é€šçŸ¥é€€å‡ºï¼šé€šè¿‡æ”¾å…¥ None è®©å„é˜Ÿåˆ—æ¶ˆè´¹è€…é€€å‡º 
        audio_face_queue.put(None)
        events_queue.put(None)
        events_dispatcher_thread.join()
        audio_worker_thread.join()
        stop_default_animation.set()
        default_animation_thread.join()
        # tcp_thread.join()
        pygame.quit()
        socket_connection.close()
        logging.info("ç³»ç»Ÿå·²é€€å‡ºã€‚")

if __name__ == "__main__":
    main()
